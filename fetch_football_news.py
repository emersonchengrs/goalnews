#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶³çƒæ–°é—» RSS Feed æŠ“å–è„šæœ¬
æŠ“å– Sky Sports Football, BBC Sport Football å’Œ The Guardian Football çš„æ–°é—»
ä½¿ç”¨ OpenAI API ç¿»è¯‘æ ‡é¢˜å¹¶è°ƒæ•´è½¬ä¼šæ–°é—»çš„è¯­æ°”
"""

import feedparser
import json
import os
import time
import requests
from datetime import datetime
from typing import List, Dict, Optional
from openai import OpenAI

# å°è¯•å¯¼å…¥ snscrapeï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    import snscrape.modules.twitter as sntwitter
    SNSCRAPE_AVAILABLE = True
except ImportError:
    SNSCRAPE_AVAILABLE = False
    print("âš ï¸  snscrape æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ RapidAPI ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆ")

# å°è¯•å¯¼å…¥å…è´¹ç¿»è¯‘åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    from deep_translator import GoogleTranslator
    try:
        from deep_translator import DeepL
    except:
        DeepL = None
    try:
        from deep_translator import LibreTranslator
    except:
        LibreTranslator = None
    FREE_TRANSLATOR_AVAILABLE = True
except ImportError:
    FREE_TRANSLATOR_AVAILABLE = False
    GoogleTranslator = None
    DeepL = None
    LibreTranslator = None


# RSS Feed URLs
RSS_FEEDS = {
    'Sky Sports': 'https://www.skysports.com/rss/football',
    'BBC Sport': 'https://feeds.bbci.co.uk/sport/football/rss.xml',
    'The Guardian': 'https://www.theguardian.com/football/rss',
    # é˜¿æ£®çº³ç›¸å…³æ–°é—»æº
    'BBC Arsenal': 'https://feeds.bbci.co.uk/sport/football/teams/arsenal/rss.xml',
    'Sky Sports Arsenal': 'https://www.skysports.com/arsenal/rss',
}

# çŸ¥åè¶³çƒè®°è€… Twitter ç”¨æˆ·å
JOURNALISTS = {
    'Fabrizio Romano': 'FabrizioRomano',
    'David Ornstein': 'David_Ornstein',  # é˜¿æ£®çº³ä¸“å®¶
    'James Pearce': 'JamesPearceLFC',
    'Chris Wheatley': 'ChrisWheatley_',  # é˜¿æ£®çº³è®°è€…
    'Gianluca Di Marzio': 'DiMarzio',
    'Charles Watts': 'charles_watts',  # é˜¿æ£®çº³è®°è€…
    'James Benge': 'jamesbenge',  # é˜¿æ£®çº³è®°è€…
    'Romano Fabrizio': 'FabrizioRomano',  # å¤‡ç”¨
}


def parse_feed(url: str, source: str) -> List[Dict]:
    """
    è§£æ RSS Feed å¹¶æå–æ–°é—»ä¿¡æ¯
    
    Args:
        url: RSS Feed URL
        source: æ–°é—»æ¥æºåç§°
    
    Returns:
        åŒ…å«æ–°é—»ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    try:
        feed = feedparser.parse(url)
        news_items = []
        
        for entry in feed.entries:
            # æå–æ ‡é¢˜
            title = entry.get('title', 'æ— æ ‡é¢˜')
            
            # æå–é“¾æ¥
            link = entry.get('link', '')
            
            # æå–å‘å¸ƒæ—¶é—´
            published_time = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                # å°†æ—¶é—´å…ƒç»„è½¬æ¢ä¸º datetime å¯¹è±¡
                published_time = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'published'):
                # å¦‚æœåªæœ‰å­—ç¬¦ä¸²æ ¼å¼çš„æ—¶é—´ï¼Œå°è¯•è§£æ
                try:
                    published_time = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %z')
                except:
                    try:
                        published_time = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')
                    except:
                        published_time = entry.published  # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²
            
            news_item = {
                'source': source,
                'title': title,
                'link': link,
                'published': published_time.isoformat() if isinstance(published_time, datetime) else str(published_time),
                'published_raw': entry.get('published', '')
            }
            
            news_items.append(news_item)
        
        return news_items
    
    except Exception as e:
        print(f"è§£æ {source} çš„ RSS Feed æ—¶å‡ºé”™: {e}")
        return []


def fetch_all_news(filter_arsenal: bool = False) -> List[Dict]:
    """
    æŠ“å–æ‰€æœ‰ RSS Feed çš„æ–°é—»
    
    Args:
        filter_arsenal: æ˜¯å¦åªæŠ“å–é˜¿æ£®çº³ç›¸å…³æ–°é—»
    
    Returns:
        æ‰€æœ‰æ–°é—»çš„åˆ—è¡¨
    """
    all_news = []
    
    # é˜¿æ£®çº³ç›¸å…³å…³é”®è¯
    arsenal_keywords = [
        'arsenal', 'gunners', 'emirates', 'arteta', 'saka', 'odegaard',
        'martinelli', 'jesus', 'saliba', 'white', 'ramsdale', 'é˜¿æ£®çº³'
    ]
    
    for source, url in RSS_FEEDS.items():
        print(f"æ­£åœ¨æŠ“å– {source} çš„æ–°é—»...")
        news_items = parse_feed(url, source)
        
        # å¦‚æœè®¾ç½®äº†è¿‡æ»¤ï¼Œåªä¿ç•™é˜¿æ£®çº³ç›¸å…³æ–°é—»
        if filter_arsenal:
            filtered_items = []
            for item in news_items:
                title_lower = item.get('title', '').lower()
                if any(keyword in title_lower for keyword in arsenal_keywords):
                    filtered_items.append(item)
            news_items = filtered_items
            print(f"  è¿‡æ»¤åé˜¿æ£®çº³ç›¸å…³æ–°é—»: {len(news_items)} æ¡")
        
        all_news.extend(news_items)
        print(f"ä» {source} è·å–äº† {len(news_items)} æ¡æ–°é—»\n")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_news.sort(key=lambda x: x.get('published', ''), reverse=True)
    
    return all_news


def translate_title_free(title: str, is_transfer: bool = False, translator_type: str = 'google') -> Dict[str, str]:
    """
    ä½¿ç”¨å…è´¹ç¿»è¯‘æœåŠ¡ç¿»è¯‘æ ‡é¢˜
    
    Args:
        title: åŸå§‹è‹±æ–‡æ ‡é¢˜
        is_transfer: æ˜¯å¦ä¸ºè½¬ä¼šæ–°é—»
        translator_type: ç¿»è¯‘æœåŠ¡ç±»å‹ ('google', 'deepl', 'libre')
    
    Returns:
        åŒ…å«ç¿»è¯‘åæ ‡é¢˜å’Œæ˜¯å¦è½¬ä¼šçš„å­—å…¸
    """
    try:
        if not FREE_TRANSLATOR_AVAILABLE:
            return {
                'title_cn': title,
                'is_transfer': is_transfer
            }
        
        # é€‰æ‹©ç¿»è¯‘æœåŠ¡
        if translator_type == 'google' and GoogleTranslator:
            translator = GoogleTranslator(source='en', target='zh-CN')
        elif translator_type == 'deepl' and DeepL:
            # DeepL éœ€è¦ API keyï¼Œä½†è¿™é‡Œå°è¯•ä½¿ç”¨å…è´¹ç‰ˆæœ¬
            try:
                translator = DeepL(source='en', target='zh', use_free_api=True)
            except:
                if GoogleTranslator:
                    translator = GoogleTranslator(source='en', target='zh-CN')
                else:
                    raise Exception("æ— æ³•ä½¿ç”¨ DeepL æˆ– Google Translator")
        elif translator_type == 'libre' and LibreTranslator:
            translator = LibreTranslator(source='en', target='zh')
        else:
            if GoogleTranslator:
                translator = GoogleTranslator(source='en', target='zh-CN')
            else:
                raise Exception("Google Translator ä¸å¯ç”¨")
        
        # ç¿»è¯‘æ ‡é¢˜ï¼ˆæ·»åŠ é‡è¯•æœºåˆ¶ï¼‰
        max_retries = 3
        translated = None
        
        for attempt in range(max_retries):
            try:
                translated = translator.translate(title)
                if translated and translated.strip():
                    break
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(1)  # ç­‰å¾…åé‡è¯•
                    continue
                else:
                    raise e
        
        if not translated or not translated.strip():
            # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œè¿”å›åŸæ ‡é¢˜
            translated = title
        
        # å¦‚æœæ˜¯è½¬ä¼šæ–°é—»ï¼Œæ·»åŠ æ¿€åŠ¨äººå¿ƒçš„è¡¨è¾¾
        if is_transfer and translated != title:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«æ¿€åŠ¨äººå¿ƒçš„è¯æ±‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
            if 'ğŸš¨' not in translated and 'é‡ç£…' not in translated and 'å®˜å®£' not in translated:
                # éšæœºæ·»åŠ ä¸€äº›æ¿€åŠ¨äººå¿ƒçš„å‰ç¼€
                prefixes = ['ğŸš¨', 'ğŸ’¥', 'âœ…']
                import random
                prefix = random.choice(prefixes)
                translated = f"{prefix} {translated}"
        
        return {
            'title_cn': translated,
            'is_transfer': is_transfer
        }
    
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œè¿”å›åŸæ ‡é¢˜
        return {
            'title_cn': title,
            'is_transfer': is_transfer
        }


def translate_title_with_ai(title: str, client: OpenAI) -> Dict[str, str]:
    """
    ä½¿ç”¨ OpenAI API ç¿»è¯‘æ ‡é¢˜å¹¶è°ƒæ•´è¯­æ°”
    
    Args:
        title: åŸå§‹è‹±æ–‡æ ‡é¢˜
        client: OpenAI å®¢æˆ·ç«¯
    
    Returns:
        åŒ…å«ç¿»è¯‘åæ ‡é¢˜å’Œæ˜¯å¦è½¬ä¼šçš„å­—å…¸
    """
    try:
        # é¦–å…ˆåˆ¤æ–­æ˜¯å¦æ˜¯è½¬ä¼šæ–°é—»
        transfer_keywords = [
            'transfer', 'sign', 'signing', 'deal', 'move', 'join', 'leave',
            'departure', 'arrival', 'agreement', 'contract', 'loan', 'permanent',
            'here we go', 'medical', 'completed', 'announced', 'confirmed'
        ]
        
        title_lower = title.lower()
        is_transfer = any(keyword in title_lower for keyword in transfer_keywords)
        
        # æ„å»ºæç¤ºè¯
        if is_transfer:
            prompt = f"""è¯·å°†ä»¥ä¸‹è¶³çƒè½¬ä¼šæ–°é—»æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶ä½¿ç”¨ Fabrizio Romano çš„æ¿€åŠ¨äººå¿ƒçš„é£æ ¼ã€‚

Fabrizio Romano çš„é£æ ¼ç‰¹ç‚¹ï¼š
- ä½¿ç”¨"Here we go!"ã€"é‡ç£…ï¼"ã€"å®˜å®£ï¼"ç­‰æ¿€åŠ¨äººå¿ƒçš„è¡¨è¾¾
- ä½¿ç”¨æ„Ÿå¹å·å’Œemojiï¼ˆå¦‚âœ…ã€ğŸš¨ã€ğŸ’¥ç­‰ï¼‰
- è¯­æ°”å…´å¥‹ã€ç›´æ¥ã€æœ‰å†²å‡»åŠ›
- çªå‡ºè½¬ä¼šçš„é‡å¤§æ€§å’Œç¡®å®šæ€§

åŸæ ‡é¢˜ï¼š{title}

è¯·åªè¿”å›ç¿»è¯‘åçš„ä¸­æ–‡æ ‡é¢˜ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Šã€‚"""
        else:
            prompt = f"""è¯·å°†ä»¥ä¸‹è¶³çƒæ–°é—»æ ‡é¢˜å‡†ç¡®ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒåŸæ„å’Œè¯­æ°”ã€‚

åŸæ ‡é¢˜ï¼š{title}

è¯·åªè¿”å›ç¿»è¯‘åçš„ä¸­æ–‡æ ‡é¢˜ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Šã€‚"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¶³çƒæ–°é—»ç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡è¶³çƒæ–°é—»ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7 if is_transfer else 0.3,
            max_tokens=200
        )
        
        translated_title = response.choices[0].message.content.strip()
        
        return {
            'title_cn': translated_title,
            'is_transfer': is_transfer
        }
    
    except Exception as e:
        print(f"ç¿»è¯‘æ ‡é¢˜æ—¶å‡ºé”™: {e}")
        return {
            'title_cn': title,  # å‡ºé”™æ—¶è¿”å›åŸæ ‡é¢˜
            'is_transfer': False
        }


def process_news_with_translation(news_items: List[Dict], 
                                  api_key: Optional[str] = None,
                                  use_free_translator: bool = False,
                                  translator_type: str = 'google') -> List[Dict]:
    """
    ä¸ºæ‰€æœ‰æ–°é—»æ·»åŠ ä¸­æ–‡ç¿»è¯‘
    
    Args:
        news_items: æ–°é—»åˆ—è¡¨
        api_key: OpenAI API å¯†é’¥ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        use_free_translator: æ˜¯å¦ä½¿ç”¨å…è´¹ç¿»è¯‘æœåŠ¡ï¼ˆé»˜è®¤ Falseï¼Œä½¿ç”¨ OpenAIï¼‰
        translator_type: å…è´¹ç¿»è¯‘æœåŠ¡ç±»å‹ ('google', 'deepl', 'libre')
    
    Returns:
        åŒ…å«ç¿»è¯‘çš„æ–°é—»åˆ—è¡¨
    """
    print("\nå¼€å§‹ç¿»è¯‘æ–°é—»æ ‡é¢˜...")
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯è½¬ä¼šæ–°é—»çš„å…³é”®è¯
    transfer_keywords = [
        'transfer', 'sign', 'signing', 'deal', 'move', 'join', 'leave',
        'departure', 'arrival', 'agreement', 'contract', 'loan', 'permanent',
        'here we go', 'medical', 'completed', 'announced', 'confirmed'
    ]
    
    # ä½¿ç”¨å…è´¹ç¿»è¯‘
    if use_free_translator or not os.getenv('OPENAI_API_KEY'):
        if not FREE_TRANSLATOR_AVAILABLE:
            print("âš ï¸  å…è´¹ç¿»è¯‘åº“æœªå®‰è£…ï¼Œè·³è¿‡ç¿»è¯‘æ­¥éª¤")
            print("   å¯ä»¥è¿è¡Œ: pip install deep-translator")
            return news_items
        
        print(f"ä½¿ç”¨å…è´¹ç¿»è¯‘æœåŠ¡: {translator_type}")
        total = len(news_items)
        
        for i, item in enumerate(news_items, 1):
            title_lower = item['title'].lower()
            is_transfer = any(keyword in title_lower for keyword in transfer_keywords)
            
            if i % 10 == 0 or i == 1:
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{total} æ¡: {item['title'][:50]}...")
            
            # ä½¿ç”¨å…è´¹ç¿»è¯‘
            translation_result = translate_title_free(item['title'], is_transfer, translator_type)
            
            # æ·»åŠ åˆ°æ–°é—»é¡¹
            item['title_cn'] = translation_result['title_cn']
            item['is_transfer'] = is_transfer
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶ï¼ˆå…è´¹æœåŠ¡é€šå¸¸æœ‰é€Ÿç‡é™åˆ¶ï¼‰
            if i < total:
                time.sleep(0.3)  # æ¯æ¬¡è¯·æ±‚é—´éš” 0.3 ç§’
        
        print(f"\nå®Œæˆï¼å…±ç¿»è¯‘äº† {total} æ¡æ–°é—»æ ‡é¢˜\n")
        return news_items
    
    # ä½¿ç”¨ OpenAI API
    if api_key is None:
        api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("âš ï¸  æœªè®¾ç½® OPENAI_API_KEYï¼Œåˆ‡æ¢åˆ°å…è´¹ç¿»è¯‘æœåŠ¡")
        return process_news_with_translation(news_items, use_free_translator=True, translator_type=translator_type)
    
    client = OpenAI(api_key=api_key)
    total = len(news_items)
    
    for i, item in enumerate(news_items, 1):
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{total} æ¡: {item['title'][:50]}...")
        
        # ç¿»è¯‘æ ‡é¢˜
        translation_result = translate_title_with_ai(item['title'], client)
        
        # æ·»åŠ åˆ°æ–°é—»é¡¹
        item['title_cn'] = translation_result['title_cn']
        item['is_transfer'] = translation_result['is_transfer']
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å… API é€Ÿç‡é™åˆ¶
        if i < total:
            time.sleep(0.5)  # æ¯æ¬¡è¯·æ±‚é—´éš” 0.5 ç§’
    
    print(f"\nå®Œæˆï¼å…±ç¿»è¯‘äº† {total} æ¡æ–°é—»æ ‡é¢˜\n")
    
    return news_items


def save_to_json(news_items: List[Dict], filename: str = 'football_news.json'):
    """
    å°†æ–°é—»ä¿å­˜åˆ° JSON æ–‡ä»¶
    
    Args:
        news_items: æ–°é—»åˆ—è¡¨
        filename: è¾“å‡ºæ–‡ä»¶å
    """
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(news_items, f, ensure_ascii=False, indent=2)
    print(f"æ–°é—»å·²ä¿å­˜åˆ° {filename}")


def fetch_tweets_with_snscrape(username: str, limit: int = 10) -> List[Dict]:
    """
    ä½¿ç”¨ snscrape è·å–æŒ‡å®šç”¨æˆ·çš„æœ€æ–°æ¨æ–‡
    
    Args:
        username: Twitter ç”¨æˆ·åï¼ˆä¸å« @ï¼‰
        limit: è·å–çš„æ¨æ–‡æ•°é‡
    
    Returns:
        æ¨æ–‡åˆ—è¡¨
    """
    tweets = []
    try:
        scraper = sntwitter.TwitterUserScraper(username)
        for i, tweet in enumerate(scraper.get_items()):
            if i >= limit:
                break
            
            tweet_data = {
                'source': f'Twitter - {username}',
                'title': tweet.rawContent[:200] if hasattr(tweet, 'rawContent') else tweet.content[:200],
                'link': tweet.url if hasattr(tweet, 'url') else f'https://twitter.com/{username}/status/{tweet.id}',
                'published': tweet.date.isoformat() if hasattr(tweet, 'date') and tweet.date else datetime.now().isoformat(),
                'published_raw': str(tweet.date) if hasattr(tweet, 'date') else '',
                'tweet_id': str(tweet.id) if hasattr(tweet, 'id') else '',
                'retweet_count': tweet.retweetCount if hasattr(tweet, 'retweetCount') else 0,
                'like_count': tweet.likeCount if hasattr(tweet, 'likeCount') else 0,
            }
            tweets.append(tweet_data)
        
        return tweets
    except Exception as e:
        print(f"ä½¿ç”¨ snscrape è·å– {username} çš„æ¨æ–‡æ—¶å‡ºé”™: {e}")
        return []


def fetch_tweets_with_rapidapi(username: str, api_key: str, limit: int = 10, api_type: str = 'auto') -> List[Dict]:
    """
    ä½¿ç”¨ RapidAPI çš„ Twitter API è·å–æŒ‡å®šç”¨æˆ·çš„æœ€æ–°æ¨æ–‡
    
    æ”¯æŒå¤šä¸ª RapidAPI Twitter API æœåŠ¡ï¼Œä¼šè‡ªåŠ¨å°è¯•å¯ç”¨çš„ API
    
    Args:
        username: Twitter ç”¨æˆ·åï¼ˆä¸å« @ï¼‰
        api_key: RapidAPI API Key
        limit: è·å–çš„æ¨æ–‡æ•°é‡
        api_type: API ç±»å‹ ('auto', 'api45', 'scraper', 'v2')ï¼Œauto ä¼šä¾æ¬¡å°è¯•
    
    Returns:
        æ¨æ–‡åˆ—è¡¨
    """
    tweets = []
    
    # å®šä¹‰å¤šä¸ª API é…ç½®
    api_configs = []
    
    if api_type == 'auto':
        # è‡ªåŠ¨æ¨¡å¼ï¼šå°è¯•æ‰€æœ‰å¯ç”¨çš„ API
        api_configs = [
            {
                'name': 'Twitter API 45',
                'url': 'https://twitter-api45.p.rapidapi.com/timeline.php',
                'host': 'twitter-api45.p.rapidapi.com',
                'params': {'screenname': username, 'count': str(limit)},
                'parse_key': 'timeline'
            },
            {
                'name': 'Twitter Scraper',
                'url': 'https://twitter-scraper-api.p.rapidapi.com/user',
                'host': 'twitter-scraper-api.p.rapidapi.com',
                'params': {'username': username, 'count': str(limit)},
                'parse_key': 'tweets'
            },
        ]
    elif api_type == 'api45':
        api_configs = [{
            'name': 'Twitter API 45',
            'url': 'https://twitter-api45.p.rapidapi.com/timeline.php',
            'host': 'twitter-api45.p.rapidapi.com',
            'params': {'screenname': username, 'count': str(limit)},
            'parse_key': 'timeline'
        }]
    elif api_type == 'scraper':
        api_configs = [{
            'name': 'Twitter Scraper',
            'url': 'https://twitter-scraper-api.p.rapidapi.com/user',
            'host': 'twitter-scraper-api.p.rapidapi.com',
            'params': {'username': username, 'count': str(limit)},
            'parse_key': 'tweets'
        }]
    
    # å°è¯•æ¯ä¸ª API é…ç½®
    for config in api_configs:
        try:
            headers = {
                "X-RapidAPI-Key": api_key,
                "X-RapidAPI-Host": config['host']
            }
            
            response = requests.get(config['url'], headers=headers, params=config['params'], timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                
                # è§£ææ•°æ®
                tweet_list = data.get(config['parse_key'], [])
                
                for tweet_data in tweet_list[:limit]:
                    # æå–æ¨æ–‡æ–‡æœ¬
                    text = tweet_data.get('text') or tweet_data.get('full_text') or tweet_data.get('content', '')
                    
                    # æå–æ¨æ–‡ ID
                    tweet_id = str(tweet_data.get('id', ''))
                    
                    # æ„å»ºé“¾æ¥
                    link = tweet_data.get('url') or f"https://twitter.com/{username}/status/{tweet_id}"
                    
                    # æå–æ—¶é—´
                    created_at = tweet_data.get('created_at') or tweet_data.get('date', datetime.now().isoformat())
                    
                    tweet = {
                        'source': f'Twitter - {username}',
                        'title': text[:200] if text else '',
                        'link': link,
                        'published': created_at if isinstance(created_at, str) else created_at.isoformat() if hasattr(created_at, 'isoformat') else datetime.now().isoformat(),
                        'published_raw': str(created_at),
                        'tweet_id': tweet_id,
                        'retweet_count': tweet_data.get('retweet_count', tweet_data.get('retweets', 0)),
                        'like_count': tweet_data.get('favorite_count', tweet_data.get('like_count', tweet_data.get('likes', 0))),
                    }
                    tweets.append(tweet)
                
                if tweets:
                    print(f"  âœ… ä½¿ç”¨ {config['name']} æˆåŠŸè·å–æ¨æ–‡")
                    return tweets
            else:
                if api_type == 'auto':
                    continue  # å°è¯•ä¸‹ä¸€ä¸ª API
                else:
                    print(f"  âŒ {config['name']} è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        
        except Exception as e:
            if api_type == 'auto':
                continue  # å°è¯•ä¸‹ä¸€ä¸ª API
            else:
                print(f"  âŒ ä½¿ç”¨ {config['name']} æ—¶å‡ºé”™: {e}")
    
    return tweets


def fetch_journalist_tweets(journalists: Optional[Dict[str, str]] = None, 
                            limit_per_journalist: int = 5,
                            use_rapidapi: bool = False,
                            rapidapi_key: Optional[str] = None) -> List[Dict]:
    """
    è·å–å¤šä¸ªçŸ¥åè®°è€…çš„æœ€æ–°æ¨æ–‡
    
    Args:
        journalists: è®°è€…å­—å…¸ {æ˜¾ç¤ºåç§°: Twitterç”¨æˆ·å}ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨
        limit_per_journalist: æ¯ä¸ªè®°è€…è·å–çš„æ¨æ–‡æ•°é‡
        use_rapidapi: æ˜¯å¦ä½¿ç”¨ RapidAPIï¼ˆå¦‚æœ snscrape ä¸å¯ç”¨æˆ–å¤±è´¥ï¼‰
        rapidapi_key: RapidAPI API Keyï¼ˆå¦‚æœä½¿ç”¨ RapidAPIï¼‰
    
    Returns:
        æ‰€æœ‰æ¨æ–‡çš„åˆ—è¡¨
    """
    if journalists is None:
        journalists = JOURNALISTS
    
    all_tweets = []
    
    print(f"\nå¼€å§‹æŠ“å–è®°è€…æ¨æ–‡...")
    print(f"ä½¿ç”¨æ–¹å¼: {'RapidAPI' if use_rapidapi or not SNSCRAPE_AVAILABLE else 'snscrape'}\n")
    
    for display_name, username in journalists.items():
        print(f"æ­£åœ¨è·å– {display_name} (@{username}) çš„æ¨æ–‡...")
        
        tweets = []
        
        # ä¼˜å…ˆå°è¯• snscrapeï¼ˆå¦‚æœå¯ç”¨ä¸”æœªå¼ºåˆ¶ä½¿ç”¨ RapidAPIï¼‰
        if SNSCRAPE_AVAILABLE and not use_rapidapi:
            tweets = fetch_tweets_with_snscrape(username, limit_per_journalist)
            
            # å¦‚æœ snscrape å¤±è´¥ï¼Œå°è¯• RapidAPI
            if not tweets and rapidapi_key:
                print(f"  snscrape å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ RapidAPI...")
                tweets = fetch_tweets_with_rapidapi(username, rapidapi_key, limit_per_journalist)
        elif use_rapidapi or not SNSCRAPE_AVAILABLE:
            if not rapidapi_key:
                rapidapi_key = os.getenv('RAPIDAPI_KEY')
            
            if rapidapi_key:
                tweets = fetch_tweets_with_rapidapi(username, rapidapi_key, limit_per_journalist)
            else:
                print(f"  âš ï¸  æœªæä¾› RapidAPI Keyï¼Œè·³è¿‡ {display_name}")
        
        if tweets:
            all_tweets.extend(tweets)
            print(f"  âœ… è·å–äº† {len(tweets)} æ¡æ¨æ–‡")
        else:
            print(f"  âŒ æœªèƒ½è·å–æ¨æ–‡")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_tweets.sort(key=lambda x: x.get('published', ''), reverse=True)
    
    print(f"\næ€»å…±è·å–äº† {len(all_tweets)} æ¡æ¨æ–‡\n")
    
    return all_tweets


def print_news(news_items: List[Dict], limit: int = 10):
    """
    æ‰“å°æ–°é—»åˆ°æ§åˆ¶å°
    
    Args:
        news_items: æ–°é—»åˆ—è¡¨
        limit: æ˜¾ç¤ºçš„æ•°é‡é™åˆ¶
    """
    print(f"\n{'='*80}")
    print(f"æœ€æ–°è¶³çƒæ–°é—» (æ˜¾ç¤ºå‰ {min(limit, len(news_items))} æ¡)")
    print(f"{'='*80}\n")
    
    for i, item in enumerate(news_items[:limit], 1):
        transfer_mark = "ğŸš¨ [è½¬ä¼š]" if item.get('is_transfer', False) else ""
        print(f"{i}. [{item['source']}] {transfer_mark}")
        print(f"   è‹±æ–‡: {item['title']}")
        print(f"   ä¸­æ–‡: {item.get('title_cn', 'æœªç¿»è¯‘')}")
        print(f"   é“¾æ¥: {item['link']}")
        print(f"   å‘å¸ƒæ—¶é—´: {item['published']}")
        print()


def main(filter_arsenal: bool = False):
    """
    ä¸»å‡½æ•°
    
    Args:
        filter_arsenal: æ˜¯å¦åªæŠ“å–é˜¿æ£®çº³ç›¸å…³æ–°é—»
    """
    print("å¼€å§‹æŠ“å–è¶³çƒæ–°é—»...\n")
    if filter_arsenal:
        print("ğŸ”´ ä»…æŠ“å–é˜¿æ£®çº³ç›¸å…³æ–°é—»\n")
    
    # æŠ“å–æ‰€æœ‰æ–°é—»
    all_news = fetch_all_news(filter_arsenal=filter_arsenal)
    
    # æŠ“å–è®°è€…æ¨æ–‡
    try:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ RapidAPI
        use_rapidapi = os.getenv('USE_RAPIDAPI', 'false').lower() == 'true'
        rapidapi_key = os.getenv('RAPIDAPI_KEY')
        
        journalist_tweets = fetch_journalist_tweets(
            limit_per_journalist=5,
            use_rapidapi=use_rapidapi,
            rapidapi_key=rapidapi_key
        )
        
        # å°†æ¨æ–‡æ·»åŠ åˆ°æ–°é—»åˆ—è¡¨ï¼ˆæ ¼å¼ç»Ÿä¸€ï¼‰
        all_news.extend(journalist_tweets)
        
        # é‡æ–°æ’åº
        all_news.sort(key=lambda x: x.get('published', ''), reverse=True)
        
    except Exception as e:
        print(f"\nâš ï¸  æŠ“å–è®°è€…æ¨æ–‡æ—¶å‡ºé”™: {e}")
        print("ç»§ç»­å¤„ç†å…¶ä»–æ–°é—»...")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\næ€»å…±è·å–äº† {len(all_news)} æ¡æ–°é—»/æ¨æ–‡")
    print(f"æ¥æºåˆ†å¸ƒ:")
    for source in RSS_FEEDS.keys():
        count = sum(1 for item in all_news if item['source'] == source)
        if count > 0:
            print(f"  - {source}: {count} æ¡")
    
    twitter_count = sum(1 for item in all_news if 'Twitter' in item.get('source', ''))
    if twitter_count > 0:
        print(f"  - Twitter æ¨æ–‡: {twitter_count} æ¡")
    
    # ç¿»è¯‘æ ‡é¢˜ï¼ˆä¼˜å…ˆä½¿ç”¨å…è´¹ç¿»è¯‘ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½® OpenAI API Keyï¼‰
    use_free = os.getenv('USE_FREE_TRANSLATOR', 'false').lower() == 'true'
    translator_type = os.getenv('TRANSLATOR_TYPE', 'google')  # google, deepl, libre
    
    # å¦‚æœæ²¡æœ‰ OpenAI API Keyï¼Œè‡ªåŠ¨ä½¿ç”¨å…è´¹ç¿»è¯‘
    if not os.getenv('OPENAI_API_KEY') or use_free:
        use_free = True
        if not os.getenv('OPENAI_API_KEY'):
            print(f"\næœªè®¾ç½® OPENAI_API_KEYï¼Œè‡ªåŠ¨ä½¿ç”¨å…è´¹ç¿»è¯‘æœåŠ¡: {translator_type}")
        else:
            print(f"\nä½¿ç”¨å…è´¹ç¿»è¯‘æœåŠ¡: {translator_type}")
    
    try:
        all_news = process_news_with_translation(
            all_news, 
            use_free_translator=use_free,
            translator_type=translator_type
        )
        
        # ç»Ÿè®¡è½¬ä¼šæ–°é—»æ•°é‡
        transfer_count = sum(1 for item in all_news if item.get('is_transfer', False))
        print(f"\nç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ€»æ–°é—»æ•°: {len(all_news)}")
        print(f"  - è½¬ä¼šæ–°é—»: {transfer_count} æ¡")
        print(f"  - å…¶ä»–æ–°é—»: {len(all_news) - transfer_count} æ¡")
        
    except ValueError as e:
        print(f"\nâš ï¸  è­¦å‘Š: {e}")
        print("è·³è¿‡ç¿»è¯‘æ­¥éª¤ï¼Œä»…ä¿å­˜åŸå§‹æ–°é—»æ•°æ®")
    except Exception as e:
        print(f"\nâŒ ç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("è·³è¿‡ç¿»è¯‘æ­¥éª¤ï¼Œä»…ä¿å­˜åŸå§‹æ–°é—»æ•°æ®")
    
    # æ˜¾ç¤ºå‰ 10 æ¡æ–°é—»
    print_news(all_news, limit=10)
    
    # ä¿å­˜åˆ° JSON æ–‡ä»¶
    save_to_json(all_news, 'football_news_translated.json')
    
    # è‡ªåŠ¨å¤åˆ¶åˆ° public ç›®å½•ä¾›ç½‘ç«™ä½¿ç”¨
    try:
        import shutil
        public_file = 'public/news.json'
        if os.path.exists('public'):
            shutil.copy2('football_news_translated.json', public_file)
            print(f"âœ… æ•°æ®å·²è‡ªåŠ¨æ›´æ–°åˆ° {public_file}")
    except Exception as e:
        print(f"âš ï¸  å¤åˆ¶åˆ° public ç›®å½•å¤±è´¥: {e}")
    
    return all_news


if __name__ == '__main__':
    import sys
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¯å¦åªæŠ“å–é˜¿æ£®çº³æ–°é—»
    filter_arsenal = '--arsenal' in sys.argv or os.getenv('FILTER_ARSENAL', 'false').lower() == 'true'
    main(filter_arsenal=filter_arsenal)

